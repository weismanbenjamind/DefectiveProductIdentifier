{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to extract features with a semantic CNN trained on the ImageNet dataset\n",
    "***\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from ConvFuncs import HDF5DatasetWriter\n",
    "import numpy as np\n",
    "import progressbar\n",
    "import random\n",
    "import os\n",
    "import jobsConfig as config\n",
    "import h5py\n",
    "from preprocessors import AspectAwarePreprocessor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, confusion_matrix\n",
    "import joblib\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_database = h5py.File(config.TEST_HDF5, \"r\")\n",
    "batch_size = 16\n",
    "buffer_size = 1000\n",
    "\n",
    "#Get data\n",
    "X_test = test_database[\"images\"]\n",
    "y_test = test_database[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with ResNet50 just like Adrian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RN50 = ResNet50(weights = \"imagenet\", include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at layers\n",
    "def inspect_model(model):\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        print(\"{}\\t{}\\t{}\".format(i, layer.output_shape, layer.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t[(None, None, None, 3)]\tInputLayer\n",
      "1\t(None, None, None, 3)\tZeroPadding2D\n",
      "2\t(None, None, None, 64)\tConv2D\n",
      "3\t(None, None, None, 64)\tBatchNormalization\n",
      "4\t(None, None, None, 64)\tActivation\n",
      "5\t(None, None, None, 64)\tZeroPadding2D\n",
      "6\t(None, None, None, 64)\tMaxPooling2D\n",
      "7\t(None, None, None, 64)\tConv2D\n",
      "8\t(None, None, None, 64)\tBatchNormalization\n",
      "9\t(None, None, None, 64)\tActivation\n",
      "10\t(None, None, None, 64)\tConv2D\n",
      "11\t(None, None, None, 64)\tBatchNormalization\n",
      "12\t(None, None, None, 64)\tActivation\n",
      "13\t(None, None, None, 256)\tConv2D\n",
      "14\t(None, None, None, 256)\tConv2D\n",
      "15\t(None, None, None, 256)\tBatchNormalization\n",
      "16\t(None, None, None, 256)\tBatchNormalization\n",
      "17\t(None, None, None, 256)\tAdd\n",
      "18\t(None, None, None, 256)\tActivation\n",
      "19\t(None, None, None, 64)\tConv2D\n",
      "20\t(None, None, None, 64)\tBatchNormalization\n",
      "21\t(None, None, None, 64)\tActivation\n",
      "22\t(None, None, None, 64)\tConv2D\n",
      "23\t(None, None, None, 64)\tBatchNormalization\n",
      "24\t(None, None, None, 64)\tActivation\n",
      "25\t(None, None, None, 256)\tConv2D\n",
      "26\t(None, None, None, 256)\tBatchNormalization\n",
      "27\t(None, None, None, 256)\tAdd\n",
      "28\t(None, None, None, 256)\tActivation\n",
      "29\t(None, None, None, 64)\tConv2D\n",
      "30\t(None, None, None, 64)\tBatchNormalization\n",
      "31\t(None, None, None, 64)\tActivation\n",
      "32\t(None, None, None, 64)\tConv2D\n",
      "33\t(None, None, None, 64)\tBatchNormalization\n",
      "34\t(None, None, None, 64)\tActivation\n",
      "35\t(None, None, None, 256)\tConv2D\n",
      "36\t(None, None, None, 256)\tBatchNormalization\n",
      "37\t(None, None, None, 256)\tAdd\n",
      "38\t(None, None, None, 256)\tActivation\n",
      "39\t(None, None, None, 128)\tConv2D\n",
      "40\t(None, None, None, 128)\tBatchNormalization\n",
      "41\t(None, None, None, 128)\tActivation\n",
      "42\t(None, None, None, 128)\tConv2D\n",
      "43\t(None, None, None, 128)\tBatchNormalization\n",
      "44\t(None, None, None, 128)\tActivation\n",
      "45\t(None, None, None, 512)\tConv2D\n",
      "46\t(None, None, None, 512)\tConv2D\n",
      "47\t(None, None, None, 512)\tBatchNormalization\n",
      "48\t(None, None, None, 512)\tBatchNormalization\n",
      "49\t(None, None, None, 512)\tAdd\n",
      "50\t(None, None, None, 512)\tActivation\n",
      "51\t(None, None, None, 128)\tConv2D\n",
      "52\t(None, None, None, 128)\tBatchNormalization\n",
      "53\t(None, None, None, 128)\tActivation\n",
      "54\t(None, None, None, 128)\tConv2D\n",
      "55\t(None, None, None, 128)\tBatchNormalization\n",
      "56\t(None, None, None, 128)\tActivation\n",
      "57\t(None, None, None, 512)\tConv2D\n",
      "58\t(None, None, None, 512)\tBatchNormalization\n",
      "59\t(None, None, None, 512)\tAdd\n",
      "60\t(None, None, None, 512)\tActivation\n",
      "61\t(None, None, None, 128)\tConv2D\n",
      "62\t(None, None, None, 128)\tBatchNormalization\n",
      "63\t(None, None, None, 128)\tActivation\n",
      "64\t(None, None, None, 128)\tConv2D\n",
      "65\t(None, None, None, 128)\tBatchNormalization\n",
      "66\t(None, None, None, 128)\tActivation\n",
      "67\t(None, None, None, 512)\tConv2D\n",
      "68\t(None, None, None, 512)\tBatchNormalization\n",
      "69\t(None, None, None, 512)\tAdd\n",
      "70\t(None, None, None, 512)\tActivation\n",
      "71\t(None, None, None, 128)\tConv2D\n",
      "72\t(None, None, None, 128)\tBatchNormalization\n",
      "73\t(None, None, None, 128)\tActivation\n",
      "74\t(None, None, None, 128)\tConv2D\n",
      "75\t(None, None, None, 128)\tBatchNormalization\n",
      "76\t(None, None, None, 128)\tActivation\n",
      "77\t(None, None, None, 512)\tConv2D\n",
      "78\t(None, None, None, 512)\tBatchNormalization\n",
      "79\t(None, None, None, 512)\tAdd\n",
      "80\t(None, None, None, 512)\tActivation\n",
      "81\t(None, None, None, 256)\tConv2D\n",
      "82\t(None, None, None, 256)\tBatchNormalization\n",
      "83\t(None, None, None, 256)\tActivation\n",
      "84\t(None, None, None, 256)\tConv2D\n",
      "85\t(None, None, None, 256)\tBatchNormalization\n",
      "86\t(None, None, None, 256)\tActivation\n",
      "87\t(None, None, None, 1024)\tConv2D\n",
      "88\t(None, None, None, 1024)\tConv2D\n",
      "89\t(None, None, None, 1024)\tBatchNormalization\n",
      "90\t(None, None, None, 1024)\tBatchNormalization\n",
      "91\t(None, None, None, 1024)\tAdd\n",
      "92\t(None, None, None, 1024)\tActivation\n",
      "93\t(None, None, None, 256)\tConv2D\n",
      "94\t(None, None, None, 256)\tBatchNormalization\n",
      "95\t(None, None, None, 256)\tActivation\n",
      "96\t(None, None, None, 256)\tConv2D\n",
      "97\t(None, None, None, 256)\tBatchNormalization\n",
      "98\t(None, None, None, 256)\tActivation\n",
      "99\t(None, None, None, 1024)\tConv2D\n",
      "100\t(None, None, None, 1024)\tBatchNormalization\n",
      "101\t(None, None, None, 1024)\tAdd\n",
      "102\t(None, None, None, 1024)\tActivation\n",
      "103\t(None, None, None, 256)\tConv2D\n",
      "104\t(None, None, None, 256)\tBatchNormalization\n",
      "105\t(None, None, None, 256)\tActivation\n",
      "106\t(None, None, None, 256)\tConv2D\n",
      "107\t(None, None, None, 256)\tBatchNormalization\n",
      "108\t(None, None, None, 256)\tActivation\n",
      "109\t(None, None, None, 1024)\tConv2D\n",
      "110\t(None, None, None, 1024)\tBatchNormalization\n",
      "111\t(None, None, None, 1024)\tAdd\n",
      "112\t(None, None, None, 1024)\tActivation\n",
      "113\t(None, None, None, 256)\tConv2D\n",
      "114\t(None, None, None, 256)\tBatchNormalization\n",
      "115\t(None, None, None, 256)\tActivation\n",
      "116\t(None, None, None, 256)\tConv2D\n",
      "117\t(None, None, None, 256)\tBatchNormalization\n",
      "118\t(None, None, None, 256)\tActivation\n",
      "119\t(None, None, None, 1024)\tConv2D\n",
      "120\t(None, None, None, 1024)\tBatchNormalization\n",
      "121\t(None, None, None, 1024)\tAdd\n",
      "122\t(None, None, None, 1024)\tActivation\n",
      "123\t(None, None, None, 256)\tConv2D\n",
      "124\t(None, None, None, 256)\tBatchNormalization\n",
      "125\t(None, None, None, 256)\tActivation\n",
      "126\t(None, None, None, 256)\tConv2D\n",
      "127\t(None, None, None, 256)\tBatchNormalization\n",
      "128\t(None, None, None, 256)\tActivation\n",
      "129\t(None, None, None, 1024)\tConv2D\n",
      "130\t(None, None, None, 1024)\tBatchNormalization\n",
      "131\t(None, None, None, 1024)\tAdd\n",
      "132\t(None, None, None, 1024)\tActivation\n",
      "133\t(None, None, None, 256)\tConv2D\n",
      "134\t(None, None, None, 256)\tBatchNormalization\n",
      "135\t(None, None, None, 256)\tActivation\n",
      "136\t(None, None, None, 256)\tConv2D\n",
      "137\t(None, None, None, 256)\tBatchNormalization\n",
      "138\t(None, None, None, 256)\tActivation\n",
      "139\t(None, None, None, 1024)\tConv2D\n",
      "140\t(None, None, None, 1024)\tBatchNormalization\n",
      "141\t(None, None, None, 1024)\tAdd\n",
      "142\t(None, None, None, 1024)\tActivation\n",
      "143\t(None, None, None, 512)\tConv2D\n",
      "144\t(None, None, None, 512)\tBatchNormalization\n",
      "145\t(None, None, None, 512)\tActivation\n",
      "146\t(None, None, None, 512)\tConv2D\n",
      "147\t(None, None, None, 512)\tBatchNormalization\n",
      "148\t(None, None, None, 512)\tActivation\n",
      "149\t(None, None, None, 2048)\tConv2D\n",
      "150\t(None, None, None, 2048)\tConv2D\n",
      "151\t(None, None, None, 2048)\tBatchNormalization\n",
      "152\t(None, None, None, 2048)\tBatchNormalization\n",
      "153\t(None, None, None, 2048)\tAdd\n",
      "154\t(None, None, None, 2048)\tActivation\n",
      "155\t(None, None, None, 512)\tConv2D\n",
      "156\t(None, None, None, 512)\tBatchNormalization\n",
      "157\t(None, None, None, 512)\tActivation\n",
      "158\t(None, None, None, 512)\tConv2D\n",
      "159\t(None, None, None, 512)\tBatchNormalization\n",
      "160\t(None, None, None, 512)\tActivation\n",
      "161\t(None, None, None, 2048)\tConv2D\n",
      "162\t(None, None, None, 2048)\tBatchNormalization\n",
      "163\t(None, None, None, 2048)\tAdd\n",
      "164\t(None, None, None, 2048)\tActivation\n",
      "165\t(None, None, None, 512)\tConv2D\n",
      "166\t(None, None, None, 512)\tBatchNormalization\n",
      "167\t(None, None, None, 512)\tActivation\n",
      "168\t(None, None, None, 512)\tConv2D\n",
      "169\t(None, None, None, 512)\tBatchNormalization\n",
      "170\t(None, None, None, 512)\tActivation\n",
      "171\t(None, None, None, 2048)\tConv2D\n",
      "172\t(None, None, None, 2048)\tBatchNormalization\n",
      "173\t(None, None, None, 2048)\tAdd\n",
      "174\t(None, None, None, 2048)\tActivation\n"
     ]
    }
   ],
   "source": [
    "inspect_model(RN50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No output layers on this network. Get a baseline by using this network as a feature extractor by dropping final FC layers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The supplied database ../data/hdf5/test_ResNet50.hdf5 already exists. Would you like to delete the database and proceede (y/n)?:  y\n"
     ]
    }
   ],
   "source": [
    "test_writer = HDF5DatasetWriter(config.TEST_FEATURES, (len(y_test), 100352), dataKey = \"features\", bufSize = buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(X, y, writer, model, batch_size, num_instances):\n",
    "    #First initialize progressbar and preprocessor\n",
    "    widgets = [\"Extracting Features: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "    pbar = progressbar.ProgressBar(maxval = num_instances, widgets = widgets).start()\n",
    "    \n",
    "    aap = AspectAwarePreprocessor(224, 224)\n",
    "    \n",
    "    #Loop through images in batches\n",
    "    for i in np.arange(0, num_instances, batch_size):\n",
    "        batch_images = X[i:i+batch_size]\n",
    "        batch_labels = y[i:i+batch_size]\n",
    "        processed_images = []\n",
    "        \n",
    "        #Preprocess each image\n",
    "        for j, image in enumerate(batch_images):\n",
    "            \n",
    "            #Ensure image is a keras compatible array\n",
    "            image = aap.preprocess(image)\n",
    "            image = img_to_array(image)\n",
    "            \n",
    "            #Preprocess image\n",
    "            image = np.expand_dims(image, axis = 0)\n",
    "            image = imagenet_utils.preprocess_input(image)\n",
    "            \n",
    "            #Replace image\n",
    "            processed_images.append(image)\n",
    "            \n",
    "        #Extract features and flatten\n",
    "        processed_images = np.vstack(processed_images)\n",
    "        features = model.predict(processed_images, batch_size = batch_size)\n",
    "        features = features.reshape((features.shape[0],100352))\n",
    "        \n",
    "        #Add features and update progressbar\n",
    "        writer.add(features, batch_labels)\n",
    "        pbar.update(i)\n",
    "        \n",
    "    #Close database and finish progressbar\n",
    "    writer.close()\n",
    "    pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100% |#####################################| Time: 0:00:35\n"
     ]
    }
   ],
   "source": [
    "extract_features(X_test, y_test, test_writer, RN50, batch_size, len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Evaluate jobs on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in dataset and model\n",
    "test = h5py.File(config.TEST_FEATURES, \"r\")\n",
    "X_test = test[\"features\"][:]\n",
    "y_test = test[\"labels\"][:]\n",
    "colorstorm = joblib.load(config.MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test set -> use 60% decision boundry\n",
    "predictions = (colorstorm.predict_proba(X_test) >= 0.60).astype(\"int\")[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       150\n",
      "           1       1.00      1.00      1.00       151\n",
      "\n",
      "    accuracy                           1.00       301\n",
      "   macro avg       1.00      1.00      1.00       301\n",
      "weighted avg       1.00      1.00      1.00       301\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Get unrounded metrics\n",
    "scores = [score(y_test, predictions) for score in [accuracy_score, precision_score, recall_score]]\n",
    "print(\"Accuracy: {}\\nPrecision: {}\\nRecall: {}\".format(scores[0], scores[1], scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[150   0]\n",
      " [  0 151]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***AI jobs is perfect on an approximately 50/50 split in labels***\n",
    "***\n",
    "## Asses jobs performance on 90/10 Split for labels\n",
    "- 301 total instances\n",
    "- Grab 271 satisfactory instances\n",
    "- Grab 80 defective instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab a 90% satisfactory and 10% defective instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indices for random instances from X_test\n",
    "sat_indices = random.sample(range(151), 90)\n",
    "def_indices = random.sample(range(150), 10)\n",
    "\n",
    "X_test_sat_90 = X_test[y_test == 1][sat_indices]\n",
    "y_test_sat_90 = y_test[y_test == 1][sat_indices]\n",
    "X_test_def_10 = X_test[y_test == 0][def_indices]\n",
    "y_test_def_10 = y_test[y_test == 0][def_indices]\n",
    "\n",
    "#Merge satisfactory/defective slices into 1\n",
    "X_test_90_10 = np.vstack([X_test_sat_90, X_test_def_10])\n",
    "y_test_90_10 = np.hstack([y_test_sat_90, y_test_def_10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "predictions_90_10 = (colorstorm.predict_proba(X_test_90_10) >= 0.60).astype(\"int\")[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        90\n",
      "\n",
      "    accuracy                           1.00       100\n",
      "   macro avg       1.00      1.00      1.00       100\n",
      "weighted avg       1.00      1.00      1.00       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show classification report\n",
    "print(classification_report(y_test_90_10, predictions_90_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Get unrounded metrics\n",
    "scores = [score(y_test_90_10, predictions_90_10) for score in [accuracy_score, precision_score, recall_score]]\n",
    "print(\"Accuracy: {}\\nPrecision: {}\\nRecall: {}\".format(scores[0], scores[1], scores[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Final Results: \n",
    "## AI jobs predicts with 100% accuracy on 50/50 satisfactory/defective instances and with 100% accuracy on 90/10 satisfactory/defective instances\n",
    "\n",
    "***Another one in the books***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
