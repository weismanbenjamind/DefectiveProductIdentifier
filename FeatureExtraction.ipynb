{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to extract features with a semantic CNN trained on the ImageNet dataset\n",
    "***\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from ConvFuncs import HDF5DatasetWriter\n",
    "import numpy as np\n",
    "import progressbar\n",
    "import random\n",
    "import os\n",
    "import jobsConfig as config\n",
    "import h5py\n",
    "from preprocessors import AspectAwarePreprocessor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_database = h5py.File(config.TRAIN_HDF5, \"r\")\n",
    "val_database = h5py.File(config.VAL_HDF5, \"r\")\n",
    "train_output = \"../data/hdf5/train_ResNet50.hdf5\"\n",
    "val_output = \"../data/hdf5/val_ResNet50.hdf5\"\n",
    "batch_size = 16\n",
    "buffer_size = 1000\n",
    "\n",
    "#Get data\n",
    "X_train = train_database[\"images\"]\n",
    "y_train = train_database[\"labels\"]\n",
    "X_val = val_database[\"images\"]\n",
    "y_val = val_database[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "833"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train[:][y_train[:] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with ResNet50 just like Adrian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RN50 = ResNet50(weights = \"imagenet\", include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at layers\n",
    "def inspect_model(model):\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        print(\"{}\\t{}\\t{}\".format(i, layer.output_shape, layer.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t[(None, None, None, 3)]\tInputLayer\n",
      "1\t(None, None, None, 3)\tZeroPadding2D\n",
      "2\t(None, None, None, 64)\tConv2D\n",
      "3\t(None, None, None, 64)\tBatchNormalization\n",
      "4\t(None, None, None, 64)\tActivation\n",
      "5\t(None, None, None, 64)\tZeroPadding2D\n",
      "6\t(None, None, None, 64)\tMaxPooling2D\n",
      "7\t(None, None, None, 64)\tConv2D\n",
      "8\t(None, None, None, 64)\tBatchNormalization\n",
      "9\t(None, None, None, 64)\tActivation\n",
      "10\t(None, None, None, 64)\tConv2D\n",
      "11\t(None, None, None, 64)\tBatchNormalization\n",
      "12\t(None, None, None, 64)\tActivation\n",
      "13\t(None, None, None, 256)\tConv2D\n",
      "14\t(None, None, None, 256)\tConv2D\n",
      "15\t(None, None, None, 256)\tBatchNormalization\n",
      "16\t(None, None, None, 256)\tBatchNormalization\n",
      "17\t(None, None, None, 256)\tAdd\n",
      "18\t(None, None, None, 256)\tActivation\n",
      "19\t(None, None, None, 64)\tConv2D\n",
      "20\t(None, None, None, 64)\tBatchNormalization\n",
      "21\t(None, None, None, 64)\tActivation\n",
      "22\t(None, None, None, 64)\tConv2D\n",
      "23\t(None, None, None, 64)\tBatchNormalization\n",
      "24\t(None, None, None, 64)\tActivation\n",
      "25\t(None, None, None, 256)\tConv2D\n",
      "26\t(None, None, None, 256)\tBatchNormalization\n",
      "27\t(None, None, None, 256)\tAdd\n",
      "28\t(None, None, None, 256)\tActivation\n",
      "29\t(None, None, None, 64)\tConv2D\n",
      "30\t(None, None, None, 64)\tBatchNormalization\n",
      "31\t(None, None, None, 64)\tActivation\n",
      "32\t(None, None, None, 64)\tConv2D\n",
      "33\t(None, None, None, 64)\tBatchNormalization\n",
      "34\t(None, None, None, 64)\tActivation\n",
      "35\t(None, None, None, 256)\tConv2D\n",
      "36\t(None, None, None, 256)\tBatchNormalization\n",
      "37\t(None, None, None, 256)\tAdd\n",
      "38\t(None, None, None, 256)\tActivation\n",
      "39\t(None, None, None, 128)\tConv2D\n",
      "40\t(None, None, None, 128)\tBatchNormalization\n",
      "41\t(None, None, None, 128)\tActivation\n",
      "42\t(None, None, None, 128)\tConv2D\n",
      "43\t(None, None, None, 128)\tBatchNormalization\n",
      "44\t(None, None, None, 128)\tActivation\n",
      "45\t(None, None, None, 512)\tConv2D\n",
      "46\t(None, None, None, 512)\tConv2D\n",
      "47\t(None, None, None, 512)\tBatchNormalization\n",
      "48\t(None, None, None, 512)\tBatchNormalization\n",
      "49\t(None, None, None, 512)\tAdd\n",
      "50\t(None, None, None, 512)\tActivation\n",
      "51\t(None, None, None, 128)\tConv2D\n",
      "52\t(None, None, None, 128)\tBatchNormalization\n",
      "53\t(None, None, None, 128)\tActivation\n",
      "54\t(None, None, None, 128)\tConv2D\n",
      "55\t(None, None, None, 128)\tBatchNormalization\n",
      "56\t(None, None, None, 128)\tActivation\n",
      "57\t(None, None, None, 512)\tConv2D\n",
      "58\t(None, None, None, 512)\tBatchNormalization\n",
      "59\t(None, None, None, 512)\tAdd\n",
      "60\t(None, None, None, 512)\tActivation\n",
      "61\t(None, None, None, 128)\tConv2D\n",
      "62\t(None, None, None, 128)\tBatchNormalization\n",
      "63\t(None, None, None, 128)\tActivation\n",
      "64\t(None, None, None, 128)\tConv2D\n",
      "65\t(None, None, None, 128)\tBatchNormalization\n",
      "66\t(None, None, None, 128)\tActivation\n",
      "67\t(None, None, None, 512)\tConv2D\n",
      "68\t(None, None, None, 512)\tBatchNormalization\n",
      "69\t(None, None, None, 512)\tAdd\n",
      "70\t(None, None, None, 512)\tActivation\n",
      "71\t(None, None, None, 128)\tConv2D\n",
      "72\t(None, None, None, 128)\tBatchNormalization\n",
      "73\t(None, None, None, 128)\tActivation\n",
      "74\t(None, None, None, 128)\tConv2D\n",
      "75\t(None, None, None, 128)\tBatchNormalization\n",
      "76\t(None, None, None, 128)\tActivation\n",
      "77\t(None, None, None, 512)\tConv2D\n",
      "78\t(None, None, None, 512)\tBatchNormalization\n",
      "79\t(None, None, None, 512)\tAdd\n",
      "80\t(None, None, None, 512)\tActivation\n",
      "81\t(None, None, None, 256)\tConv2D\n",
      "82\t(None, None, None, 256)\tBatchNormalization\n",
      "83\t(None, None, None, 256)\tActivation\n",
      "84\t(None, None, None, 256)\tConv2D\n",
      "85\t(None, None, None, 256)\tBatchNormalization\n",
      "86\t(None, None, None, 256)\tActivation\n",
      "87\t(None, None, None, 1024)\tConv2D\n",
      "88\t(None, None, None, 1024)\tConv2D\n",
      "89\t(None, None, None, 1024)\tBatchNormalization\n",
      "90\t(None, None, None, 1024)\tBatchNormalization\n",
      "91\t(None, None, None, 1024)\tAdd\n",
      "92\t(None, None, None, 1024)\tActivation\n",
      "93\t(None, None, None, 256)\tConv2D\n",
      "94\t(None, None, None, 256)\tBatchNormalization\n",
      "95\t(None, None, None, 256)\tActivation\n",
      "96\t(None, None, None, 256)\tConv2D\n",
      "97\t(None, None, None, 256)\tBatchNormalization\n",
      "98\t(None, None, None, 256)\tActivation\n",
      "99\t(None, None, None, 1024)\tConv2D\n",
      "100\t(None, None, None, 1024)\tBatchNormalization\n",
      "101\t(None, None, None, 1024)\tAdd\n",
      "102\t(None, None, None, 1024)\tActivation\n",
      "103\t(None, None, None, 256)\tConv2D\n",
      "104\t(None, None, None, 256)\tBatchNormalization\n",
      "105\t(None, None, None, 256)\tActivation\n",
      "106\t(None, None, None, 256)\tConv2D\n",
      "107\t(None, None, None, 256)\tBatchNormalization\n",
      "108\t(None, None, None, 256)\tActivation\n",
      "109\t(None, None, None, 1024)\tConv2D\n",
      "110\t(None, None, None, 1024)\tBatchNormalization\n",
      "111\t(None, None, None, 1024)\tAdd\n",
      "112\t(None, None, None, 1024)\tActivation\n",
      "113\t(None, None, None, 256)\tConv2D\n",
      "114\t(None, None, None, 256)\tBatchNormalization\n",
      "115\t(None, None, None, 256)\tActivation\n",
      "116\t(None, None, None, 256)\tConv2D\n",
      "117\t(None, None, None, 256)\tBatchNormalization\n",
      "118\t(None, None, None, 256)\tActivation\n",
      "119\t(None, None, None, 1024)\tConv2D\n",
      "120\t(None, None, None, 1024)\tBatchNormalization\n",
      "121\t(None, None, None, 1024)\tAdd\n",
      "122\t(None, None, None, 1024)\tActivation\n",
      "123\t(None, None, None, 256)\tConv2D\n",
      "124\t(None, None, None, 256)\tBatchNormalization\n",
      "125\t(None, None, None, 256)\tActivation\n",
      "126\t(None, None, None, 256)\tConv2D\n",
      "127\t(None, None, None, 256)\tBatchNormalization\n",
      "128\t(None, None, None, 256)\tActivation\n",
      "129\t(None, None, None, 1024)\tConv2D\n",
      "130\t(None, None, None, 1024)\tBatchNormalization\n",
      "131\t(None, None, None, 1024)\tAdd\n",
      "132\t(None, None, None, 1024)\tActivation\n",
      "133\t(None, None, None, 256)\tConv2D\n",
      "134\t(None, None, None, 256)\tBatchNormalization\n",
      "135\t(None, None, None, 256)\tActivation\n",
      "136\t(None, None, None, 256)\tConv2D\n",
      "137\t(None, None, None, 256)\tBatchNormalization\n",
      "138\t(None, None, None, 256)\tActivation\n",
      "139\t(None, None, None, 1024)\tConv2D\n",
      "140\t(None, None, None, 1024)\tBatchNormalization\n",
      "141\t(None, None, None, 1024)\tAdd\n",
      "142\t(None, None, None, 1024)\tActivation\n",
      "143\t(None, None, None, 512)\tConv2D\n",
      "144\t(None, None, None, 512)\tBatchNormalization\n",
      "145\t(None, None, None, 512)\tActivation\n",
      "146\t(None, None, None, 512)\tConv2D\n",
      "147\t(None, None, None, 512)\tBatchNormalization\n",
      "148\t(None, None, None, 512)\tActivation\n",
      "149\t(None, None, None, 2048)\tConv2D\n",
      "150\t(None, None, None, 2048)\tConv2D\n",
      "151\t(None, None, None, 2048)\tBatchNormalization\n",
      "152\t(None, None, None, 2048)\tBatchNormalization\n",
      "153\t(None, None, None, 2048)\tAdd\n",
      "154\t(None, None, None, 2048)\tActivation\n",
      "155\t(None, None, None, 512)\tConv2D\n",
      "156\t(None, None, None, 512)\tBatchNormalization\n",
      "157\t(None, None, None, 512)\tActivation\n",
      "158\t(None, None, None, 512)\tConv2D\n",
      "159\t(None, None, None, 512)\tBatchNormalization\n",
      "160\t(None, None, None, 512)\tActivation\n",
      "161\t(None, None, None, 2048)\tConv2D\n",
      "162\t(None, None, None, 2048)\tBatchNormalization\n",
      "163\t(None, None, None, 2048)\tAdd\n",
      "164\t(None, None, None, 2048)\tActivation\n",
      "165\t(None, None, None, 512)\tConv2D\n",
      "166\t(None, None, None, 512)\tBatchNormalization\n",
      "167\t(None, None, None, 512)\tActivation\n",
      "168\t(None, None, None, 512)\tConv2D\n",
      "169\t(None, None, None, 512)\tBatchNormalization\n",
      "170\t(None, None, None, 512)\tActivation\n",
      "171\t(None, None, None, 2048)\tConv2D\n",
      "172\t(None, None, None, 2048)\tBatchNormalization\n",
      "173\t(None, None, None, 2048)\tAdd\n",
      "174\t(None, None, None, 2048)\tActivation\n"
     ]
    }
   ],
   "source": [
    "inspect_model(RN50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No output layers on this network. Get a baseline by using this network as a feature extractor by dropping final FC layers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The supplied database ../data/hdf5/train_ResNet50.hdf5 already exists. Would you like to delete the database and proceede (y/n)?:  y\n"
     ]
    }
   ],
   "source": [
    "train_writer = HDF5DatasetWriter(train_output, (len(y_train), 100352), dataKey = \"features\", bufSize = buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(X, y, writer, model, batch_size, num_instances):\n",
    "    #First initialize progressbar and preprocessor\n",
    "    widgets = [\"Extracting Features: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "    pbar = progressbar.ProgressBar(maxval = num_instances, widgets = widgets).start()\n",
    "    \n",
    "    aap = AspectAwarePreprocessor(224, 224)\n",
    "    \n",
    "    #Loop through images in batches\n",
    "    for i in np.arange(0, num_instances, batch_size):\n",
    "        batch_images = X[i:i+batch_size]\n",
    "        batch_labels = y[i:i+batch_size]\n",
    "        processed_images = []\n",
    "        \n",
    "        #Preprocess each image\n",
    "        for j, image in enumerate(batch_images):\n",
    "            \n",
    "            #Ensure image is a keras compatible array\n",
    "            image = aap.preprocess(image)\n",
    "            image = img_to_array(image)\n",
    "            \n",
    "            #Preprocess image\n",
    "            image = np.expand_dims(image, axis = 0)\n",
    "            image = imagenet_utils.preprocess_input(image)\n",
    "            \n",
    "            #Replace image\n",
    "            processed_images.append(image)\n",
    "            \n",
    "        #Extract features and flatten\n",
    "        processed_images = np.vstack(processed_images)\n",
    "        features = model.predict(processed_images, batch_size = batch_size)\n",
    "        features = features.reshape((features.shape[0],100352))\n",
    "        \n",
    "        #Add features and update progressbar\n",
    "        writer.add(features, batch_labels)\n",
    "        pbar.update(i)\n",
    "        \n",
    "    #Close database and finish progressbar\n",
    "    writer.close()\n",
    "    pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100% |#####################################| Time: 0:02:40\n"
     ]
    }
   ],
   "source": [
    "extract_features(X_train, y_train, train_writer, RN50, batch_size, len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_writer = HDF5DatasetWriter(val_output, (len(y_val), 100352), dataKey = \"features\", bufSize = buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100% |#####################################| Time: 0:00:51\n"
     ]
    }
   ],
   "source": [
    "extract_features(X_val, y_val, val_writer, RN50, batch_size, len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Train logistic regression model on features extracted by ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in datasets\n",
    "train = h5py.File(\"../data/hdf5/train_ResNet50.hdf5\", \"r\")\n",
    "val = h5py.File(\"../data/hdf5/val_ResNet50.hdf5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\OCInstalls\\Anaconda\\envs\\jobs\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\OCInstalls\\Anaconda\\envs\\jobs\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\OCInstalls\\Anaconda\\envs\\jobs\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\OCInstalls\\Anaconda\\envs\\jobs\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\OCInstalls\\Anaconda\\envs\\jobs\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\OCInstalls\\Anaconda\\envs\\jobs\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\OCInstalls\\Anaconda\\envs\\jobs\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\OCInstalls\\Anaconda\\envs\\jobs\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\OCInstalls\\Anaconda\\envs\\jobs\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   16.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   19.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\OCInstalls\\Anaconda\\envs\\jobs\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   18.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\OCInstalls\\Anaconda\\envs\\jobs\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   16.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(random_state=42, verbose=1),\n",
       "             param_grid={'C': [0.0001, 0.001, 0.01, 1, 10],\n",
       "                         'max_iter': [100, 500]})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grid search for best logistic regressor\n",
    "log_reg = LogisticRegression(random_state = 42, verbose = 1)\n",
    "grid = {\"C\": [0.0001, 0.001, 0.01, 1, 10], \"max_iter\":[100,500]}\n",
    "log_grid = GridSearchCV(log_reg, grid, cv = 5)\n",
    "log_grid.fit(train[\"features\"][:], train[\"labels\"][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9963963963963964"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_best = log_grid.best_estimator_\n",
    "preds = log_reg_best.predict(val[\"features\"][:])\n",
    "score = accuracy_score(val[\"labels\"][:],preds)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/colorstrom.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(log_reg_best, \"output/colorstrom.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'max_iter': 100}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_grid.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
